{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Generation from Images CNN & RNN キャプション生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn-images-1.medium.com/max/1200/0*mCHDMNdwb_gB1Rj9.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "与えられた画像の説明文（キャプション）の作成を自動化するAI\n",
    "### Architecture\n",
    "CNN & LSTM  転移学習\n",
    "### Framework\n",
    "Keras ( TensorFlow backend )\n",
    "### Reserch Paper\n",
    "Where to put the Image in an Image Caption Generator (https://arxiv.org/abs/1703.09137)  \n",
    "Oxford Visual Geometry Group, or VGG, model that won the ImageNet competition in 2014   \n",
    "Very Deep Convolutional Networks for Large-Scale Visual Recognition (http://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    "### Process\n",
    "0. タスク定義\n",
    "1. データ準備（Flickr 8k）\n",
    "2. ニューラルネット構築（転移学習）\n",
    "3. 学習&テスト\n",
    "4. 評価&改善（BLEU scores）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 写真の内容を解釈するために、事前に訓練されたモデルを使用します。\n",
    "- その後、これらの機能を後でロードして、データセット内の特定の写真の解釈としてモデルに入力できます。  \n",
    "- VGGクラスを使用して、KerasにVGGモデルをロードできます。\n",
    "- これは写真の分類を予測するために使用されるモデルなので、ロードされたモデルから最後のレイヤーを削除します。\n",
    "- 画像の分類には興味がありませんが、分類が行われる直前の写真の内部表現には興味があります。\n",
    "- これらは、モデルが写真から抽出した「特徴」です。\n",
    "- Kerasはまた、ロードした写真をモデルに適したサイズに変形するためのツールも提供します（例：3チャンネル224 x 224ピクセルの画像）。\n",
    "- 下はextract_features（）という名前の関数です。\n",
    "- ディレクトリ名を指定すると、各写真をロードしてVGG用に準備し、VGGモデルから予測された特徴を収集します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Prepare Text Data 画像から特徴量を生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16 is a CNN architecture named Visual Geometry Group from Oxford\n",
    "\n",
    "\n",
    "##### 大量の画像データに対して、VGG16を用いた転移学習を実行する\n",
    "\n",
    "- 転移学習：既存の学習済モデル（出力層以外の部分）を、重みデータは変更せずに特徴量抽出機として利用する\n",
    "- ファインチューニング：既存の学習済モデル（出力層以外の部分）を、重みデータを一部再学習して特徴量抽出機として利用する\n",
    "  \n",
    "###### 転移学習の手順\n",
    "1. 入力画像から、特徴量(ボトルネック特徴量)を抽出する\n",
    "2. ボトルネック特徴量を用いて、クラス分類をする\n",
    "  \n",
    "　つまり転移学習では、VGG16など大規模なデータを用いて学習した強力なモデルを特徴抽出器として利用し（多数の対象を分類できる為、画像の特徴を捉えるのが非常に上手い）、任意のクラスの分類する為の特徴量の圧縮器として利用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(curr_dir):\n",
    "    # CNNの一つであるVGG16モデルで転移学習を行う\n",
    "    model = VGG16() \n",
    "    # モデルの再構築\n",
    "    model.layers.pop()\n",
    "    model = Model( inputs = model.inputs, outputs = model.layers[-1].output )\n",
    "    # モデルの統合\n",
    "    print(model.summary())\n",
    "    \n",
    "    features = dict()\n",
    "    # 各画像から特徴を生成する\n",
    "    for name in listdir(curr_dir):\n",
    "        filename = curr_dir + \"/\" + name\n",
    "        image = load_img( filename, target_size=(224, 224) )\n",
    "        # 計算の高速化のために画像のピクセル数をndarrayに変換する\n",
    "        image = img_to_array(image)\n",
    "        # 画像データを4次元アレイに変換する\n",
    "        image = image.reshape(( 1, image.shape[0], image.shape[1], image.shape[2] ))\n",
    "        # VGG16モデルへ入力するため、画像データを最終処理\n",
    "        image = preprocess_input(image)\n",
    "        # 特徴量を生成 ( verbose: 0, 1または2．詳細表示モード．0とすると標準出力にログを出力しません )\n",
    "        feature = model.predict( image, verbose = 0 )\n",
    "        # 画像のIDを生成\n",
    "        image_id = name.split(\".\")[0]\n",
    "        features[image_id] = feature\n",
    "        print( \">%s\"%name )\n",
    "    \n",
    "    # 戻り値はkey=画像ID, value=画像の特徴量という辞書 \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の関数にデータを流し込んで特徴量生成を実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = \"/Users/akr712/Desktop/CaptionGeneration_CNNandLSTM_Keras/Flicker8k_Dataset\"\n",
    "features = extract_features(curr_dir)\n",
    "print( \"生成された特徴量の次元数: %d\" % len(features) )\n",
    "# Pickleモジュールはプログラムを実行し終えたあとも作成したオブジェクトを保存する機能を提供してくれる\n",
    "dump( features, open(\"features.pkl\", \"wb\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Prepare Text Data テキストから特徴量を生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filename):\n",
    "    \"\"\"\n",
    "    function: ファイル内のテキストを抽出する\n",
    "    input: テキストファイル\n",
    "    output: テキスト\n",
    "    \"\"\"\n",
    "    file = open(filename, \"r\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "filename = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "texts = load_text(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CNN LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Data の中身はこうなっています\n",
    "  \n",
    "\n",
    "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .  \n",
    "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .  \n",
    "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .  \n",
    "1000268201_693b08cb0e.jpg#3\tA little girl climbing the stairs to her playhouse .  \n",
    "1000268201_693b08cb0e.jpg#4\tA little girl in a pink dress going into a wooden cabin .  \n",
    "1001773457_577c3a7d70.jpg#0\tA black dog and a spotted dog are fighting  \n",
    "1001773457_577c3a7d70.jpg#1\tA black dog and a tri-colored dog playing with each other on the road .  \n",
    "1001773457_577c3a7d70.jpg#2\tA black dog and a white dog with brown spots are staring at each other in the street .  \n",
    "1001773457_577c3a7d70.jpg#3\tTwo dogs of different breeds looking at each other on the road .  \n",
    "1001773457_577c3a7d70.jpg#4\tTwo dogs on pavement moving toward each other .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_descriptions(text):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    mapping = dict()\n",
    "    \n",
    "    for line in text.split(\"\\n\"):\n",
    "        tokens = line.split()\n",
    "        # descriptionがないものは除外\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        image_id, image_desc = tokens[0], [1:]\n",
    "        image_id = image_id.split(\".\")[0]\n",
    "        image_desc = \" \".join(image_desc)\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        mapping[image_id].append(image_desc)\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "descriptions = extract_descriptions(texts)\n",
    "print(\"ロード中: %d\" % len(descriptions) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    table = str.maketrans(\", \". string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        \n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            desc = desc.split()\n",
    "            desc = [ word.lower(), for word in desc ]\n",
    "            desc = [ punc.translate(table) for punc in desc ]\n",
    "            desc = [ word for word in desc len(word) > 1 ]\n",
    "            desc = [ word for word in desc if word.isalpha() ]\n",
    "            desc_list[i] = \"\".join(desc)\n",
    "            \n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc2vocab(descriptions):\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        for d in descriptions[key]:\n",
    "            all_desc.update(d.split())\n",
    "    return all_desc\n",
    "\n",
    "vocabulary = desc2vocab(descriptions)\n",
    "print(\"Vocabulary Size: %d\" % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、画像IDとDiscriptionを含む辞書をdescriptions.txtという新しいファイルに保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append( key + \" \" + desc )\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename, \"w\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "save_descriptions(descriptions, descriptions.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Develop CNN-LSTM Model ニューラルネットのアーキテクチャを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filename):\n",
    "    \"\"\"\n",
    "    function: ファイル内のテキストを抽出する\n",
    "    input: テキストファイル\n",
    "    output: テキスト\n",
    "    \"\"\"\n",
    "    file = open(filename, \"r\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_data(filename):\n",
    "    text = load_text(filename)\n",
    "    data = list()\n",
    "    \n",
    "    for line in text.split(\"\\n\"):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        identifier = line.split(\".\")[0]\n",
    "        data.append(identifier)\n",
    "    \n",
    "    return set(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_description(filename, data):\n",
    "    text = load_text(filename)\n",
    "    descriptions = dict()\n",
    "    \n",
    "    for line in text.split(\"\\n\"):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        \n",
    "        if image_id not in descriptions:\n",
    "            descriptions[image_id] = list()\n",
    "        desc =  'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "        descriptions[image_id].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像から特徴量を抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "def extract_image_features(filename, data):\n",
    "    all_features = load(open(filename, \"rb\"))\n",
    "    features = { k: all_features[k] for k in  data }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像とでディスクリプションを出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Flickr8k_text/Flickr_8k.trainImages.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a75abb9d8f28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Flickr8k_text/Flickr_8k.trainImages.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 画像の特徴量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bc3fd290b47a>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bc3fd290b47a>\u001b[0m in \u001b[0;36mload_text\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mテキスト\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Flickr8k_text/Flickr_8k.trainImages.txt'"
     ]
    }
   ],
   "source": [
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_data(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "\n",
    "# 画像の特徴量\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Images: train=%d' % len(train_features))\n",
    "\n",
    "# ディスクリプション\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc2line(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [ all_desc.append(d) for d in descriptions[key] ]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_descriptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cc2fa784f4fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc2token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"単語数 : %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_descriptions' is not defined"
     ]
    }
   ],
   "source": [
    "def desc2token(descriptions):\n",
    "    lines = desc2line(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = desc2token(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"単語数 : %d\" % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像についての文章を作成して、キャプションを生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(tokenizer, max_length, descriptions, images):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # それぞれの画像をループ\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # 画像のデスクリプションをループ\n",
    "        for desc in desc_list:\n",
    "            # デスクリプションをエンコード\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                # 入力文と出力文をペアにする\n",
    "                in_seq, out_seq =seq[:i], seq[i]\n",
    "                # 文を固定長に合わせる\n",
    "                in_seq = pad_sequences( [in_seq], maxlen = max_length )[0]\n",
    "                # 出力文をエンコードする\n",
    "                out_seq = to_categorical( [out_seq], num_classes=vocab_size )[0]\n",
    "                # 保存する\n",
    "                X1.append(images[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    \n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最も単語数の多いデスクリプションの単語数をカウントする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(desceiptions):\n",
    "    lines = desc2line(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model :  “merge-model” described by Marc Tanti\n",
    "- Where to put the Image in an Image Caption Generator, 2017.\n",
    "- What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?, 2017. https://arxiv.org/abs/1708.02043\n",
    "   \n",
    "![title](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Schematic-of-the-Merge-Model-For-Image-Captioning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Plot-of-the-Caption-Generation-Deep-Learning-Model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Photo Feature Extractor. This is a 16-layer VGG model pre-trained on the ImageNet dataset. We have pre-processed the photos with the VGG model (without the output layer) and will use the extracted features predicted by this model as input.\n",
    "- Sequence Processor. This is a word embedding layer for handling the text input, followed by a Long Short-Term Memory (LSTM) recurrent neural network layer.\n",
    "- Decoder (for lack of a better name). Both the feature extractor and sequence processor output a fixed-length vector. These are merged together and processed by a Dense layer to make a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "キャプションを生成する関数を定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_generator(vocab_size, max_length):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # 特徴量を生成\n",
    "    input1 = Input(shape = (4096,))\n",
    "    feature1 = Dropout(0.5)(input1)\n",
    "    feature2 = Dense(256, activation=\"relu\")(feature1)\n",
    "    \n",
    "    # LSTMに入力するシーケンスに変換する\n",
    "    input2 = Input(shape = (max_length,))\n",
    "    seq1 = Embedding(vocab_size, 256, mask_zero=True)(input2)\n",
    "    seq2 = Dropout(0.5)(seq1)\n",
    "    seq3 = LSTM(256)(seq2)\n",
    "    \n",
    "    # decoderを生成\n",
    "    decoder1 = add([feature2, seq3])\n",
    "    decoder2 = Dense(256, activation=\"relu\")(decoder1)\n",
    "    output = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # [画像、ディスクリプション]と[単語]を結びつける\n",
    "    model = Model( inputs=[input1, input2], outputs=output )\n",
    "    # LSTMの出力層の活性化関数はAdam\n",
    "    model.compile( loss = \"'categorical_crossentropy\", optimizer = \"adam\" )\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file=\"model.png\", show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チェックポイントを作る\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1f573b11237a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX1train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 学習スタート\n",
    "model.fit( [X1train, X2train], ytrain, epochs = 20, verbose = 2, callbacks = [checkpoint], validation_data = ([X1test, X2test], ytest) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
